<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="main.css">
    <link rel="icon" type="image/x-icon" href="assets/favicon.png">
    <title>Real-World Humanoid Locomotion with Reinforcement Learning</title>
</head>
<body>
<div id="video_grid">
  <div class="video_wrapper">
    <div class="video_container">
      <video autoplay muted playsinline loop>
        <source src="assets/grid/backview_sproul.mp4" type="video/mp4">
      </video>
    </div>
  </div>
  <div class="video_wrapper">
    <div class="video_container">
      <video autoplay muted playsinline loop>
        <source src="assets/grid/stadium_cal.mp4" type="video/mp4">
      </video>
    </div>
  </div>
  <div class="video_wrapper">
    <div class="video_container">
      <video autoplay muted playsinline loop>
        <source src="assets/grid/grass_backwards.mp4" type="video/mp4">
      </video>
    </div>
  </div>
  <div class="video_wrapper">
    <div class="video_container">
      <video autoplay muted playsinline loop>
        <source src="assets/grid/sather_tower_turning.mp4" type="video/mp4">
      </video>
    </div>
  </div>
  <div class="video_wrapper">
    <div class="video_container">
      <video autoplay muted playsinline loop>
        <source src="assets/grid/bridge.mp4" type="video/mp4">
      </video>
    </div>
  </div>
  <div class="video_wrapper">
    <div class="video_container">
      <video autoplay muted playsinline loop>
        <source src="assets/grid/sather_gate_2.mp4" type="video/mp4">
      </video>
    </div>
  </div>
  <div class="video_wrapper">
    <div class="video_container">
      <video autoplay muted playsinline loop>
        <source src="assets/grid/crossroad.mp4" type="video/mp4">
      </video>
    </div>
  </div>
  <div class="video_wrapper">
    <div class="video_container">
      <video autoplay muted playsinline loop>
        <source src="assets/grid/yoga_ball.mp4" type="video/mp4">
      </video>
    </div>
  </div>
  <div class="video_wrapper">
    <div class="video_container">
      <video autoplay muted playsinline loop>
        <source src="assets/grid/sather_tower_walking.mp4" type="video/mp4">
      </video>
    </div>
  </div>
  <div class="video_wrapper">
    <div class="video_container">
      <video autoplay muted playsinline loop>
        <source src="assets/grid/sather_gate_1.mp4" type="video/mp4">
      </video>
    </div>
  </div>
  <div class="video_wrapper">
    <div class="video_container">
      <video autoplay muted playsinline loop>
        <source src="assets/grid/PSR.mp4" type="video/mp4">
      </video>
    </div>
  </div>
  <div class="video_wrapper">
    <div class="video_container">
      <video autoplay muted playsinline loop>
        <source src="assets/grid/le_conte.mp4" type="video/mp4">
      </video>
    </div>
  </div>
  <div class="video_wrapper">
    <div class="video_container">
      <video autoplay muted playsinline loop>
        <source src="assets/grid/path_before_bridge.mp4" type="video/mp4">
      </video>
    </div>
  </div>
  <div class="video_wrapper">
    <div class="video_container">
      <video autoplay muted playsinline loop>
        <source src="assets/grid/the_other_crossing.mp4" type="video/mp4">
      </video>
    </div>
  </div>
  <div class="video_wrapper">
    <div class="video_container">
      <video autoplay muted playsinline loop>
        <source src="assets/grid/in_front_of_garage.mp4" type="video/mp4">
      </video>
    </div>
  </div>
  <div class="video_wrapper">
    <div class="video_container">
      <video autoplay muted playsinline loop>
        <source src="assets/grid/stop_sign.mp4" type="video/mp4">
      </video>
    </div>
  </div>
</div>
<div id="title_slide">
    <div class="title_left">
        <h1>Real-World Humanoid Locomotion <br> with Reinforcement Learning</h1>
        <div class="author-container">
            <div class="grid-item"><a href="https://people.eecs.berkeley.edu/~ilija/">Ilija Radosavovic<sup>*</sup></a></div>
            <div class="grid-item"><a href="https://tetexiao.com/">Tete Xiao<sup>*</sup></a></div>
            <div class="grid-item"><a href="https://bikezhang106.github.io/">Bike Zhang<sup>*</sup></a></div>
            <div class="grid-item"><a href="http://people.eecs.berkeley.edu/~trevor/">Trevor Darrell<sup>†</sup></a></div>
            <div class="grid-item"><a href="http://people.eecs.berkeley.edu/~malik/">Jitendra Malik<sup>†</sup></a></div>
            <div class="grid-item"><a href="https://hybrid-robotics.berkeley.edu/koushil/">Koushil Sreenath<sup>†</sup></a></div>
        </div>
        <div class="mobile-author-container-1">
            <div class="grid-item"><a href="https://people.eecs.berkeley.edu/~ilija/">Ilija Radosavovic*</a></div>
            <div class="grid-item"><a href="https://tetexiao.com/">Tete Xiao*</a></div>
            <div class="grid-item"><a href="https://bikezhang106.github.io/">Bike Zhang*</a></div>
        </div>
        <div class="mobile-author-container-2">
            <div class="grid-item"><a href="http://people.eecs.berkeley.edu/~trevor/">Trevor Darrell<sup>†</sup></a></div>
            <div class="grid-item"><a href="http://people.eecs.berkeley.edu/~malik/">Jitendra Malik<sup>†</sup></a></div>
            <div class="grid-item"><a href="https://hybrid-robotics.berkeley.edu/koushil/">Koushil Sreenath<sup>†</sup></a></div>
        </div>
        <div class="contribution">
            Authors contributed equally and listed in alphabetical order</p>
        </div>
        <div class="berkeley">
            <p>University of California, Berkeley</p>
        </div>
        <div class="button-container">
            <a href="assets/paper.pdf" class="button">Paper</a>
            <a href="https://youtu.be/eFoBfFhwo18" class="button">Video</a>
        </div>

        <br>

        <div id="abstract" class="grid-container">
            <p>
            Humanoid robots that can autonomously operate in diverse environments have the potential to help address labour shortages in factories, assist elderly at homes, and colonize new planets. While classical controllers for humanoid robots have shown impressive results in a number of settings, they are challenging to generalize and adapt to new environments. Here, we present a fully learning-based approach for real-world humanoid locomotion. Our controller is a causal Transformer that takes the history of proprioceptive observations and actions as input and predicts the next action. We hypothesize that the observation-action history contains useful information about the world that a powerful Transformer model can use to adapt its behavior in-context, without updating its weights. We train our model with large-scale model-free reinforcement learning on an ensemble of randomized environments in simulation and deploy it to the real world zero-shot. Our controller can walk over various outdoor terrains, is robust to external disturbances, and can adapt in context.
            </p>
        </div>
    </div>
</div>
<hr class="rounded">
<div id="overview">

    <h1>Learning Humanoid Locomotion</h1>

    <p>
        We present a learning-based approach for humanoid locomotion. Our controller is a Transformer that predicts future actions autoregressively from the history of past observations and actions.  
        We hypothesize that the history of observations and actions implicitly encodes the information about the world that a powerful Transformer model can use to adapt its behavior dynamically at test time. For example, the model can use the history of desired vs actual states to figure out how to adjust its actions to better achieve future states. This can be seen as a form of in-context learning—changing model behavior without updating the model parameters.
    </p>

    <div class="approach">
        <div class="video_container">
            <video loop autoplay muted playsinline preload="metadata">
                <source src="assets/inference.m4v" type="video/mp4">
            </video>
            <div class="caption" style="margin-top: -2.0vw">
                <p> Our controller is a causal Transformer trained by autoregressive prediction of future actions from the observation-action history.</p>
            </div>
        </div>
    </div>

    <h1>Massively Parallel Training in Simulation</h1>
    <p>
        Our model is trained with large-scale model-free reinforcement learning (RL) on an ensemble of randomized environments in simulation.
        We leverage fast GPU simulation powered by IsaacGym and parallelize training across multiple GPUs and thousands of environments.
        Thanks to this, we are able to collect a large number of samples for training (order of 10 Billion in about a day).
    </p>
    <div class="allegrofail">
        <div class="video_container">
            <video loop autoplay muted playsinline preload="metadata">
                <source src="assets/sim.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p> We train our policies on various terrain types, including planes, rough planes, and smooth slopes. Our robots execute a variety of randomly sampled walking commands such as walking forward, sideward, turning, or a combination thereof. </p>
            </div>
        </div>
    </div>

    <h1>Real-World Deployment</h1>

    <p>
    We find that our policies trained entirely in simulation are able to transfer to the real world zero-shot.
    We deploy our controller to a number of outdoor environments. These include plazas, walkways, sidewalks, running tracks, and grass fields. The terrains vary considerably in terms of material properties, like concrete, rubber, and grass, as well as conditions, like dry or wet.
    </p>
    <br>

    <div class="allegroupper">
        <video autoplay muted playsinline loop preload="metadata">
            <source src="assets/content/sather_gate_2_v3.mp4" type="video/mp4">
        </video>
        <video autoplay muted playsinline loop preload="metadata">
            <source src="assets/content/sather_gate_1_v3.mp4" type="video/mp4">
        </video>
    </div>
    <div class="allegrolower">
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/content/bridge_v2.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>Walking in different outdoor environments</p>
            </div>
        </div>
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/content/crossroad_v2.mp4" type="video/mp4">
            </video>
        </div>
    </div>

    <h1>Omnidirectional Walking</h1>

    <p>
    Our controller is able to accurately follow a range of velocity commands to perform omni-directional locomotion, including  walking forward, backward, and turning.
    </p>

    <br>
    <div class="allegrolower">
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/content/grass_backwards_1.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>Walking backward and turning</p>
            </div>
        </div>
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/content/sather_tower_turning_v3.mp4" type="video/mp4">
            </video>
        </div>
    </div>

    <h1>Dynamic Arm Swing</h1>

    <p>
    We find that our approach leads to emergent human-like dynamic arm swing behaviors in coordination with leg movements, i.e., a contralateral relationship between the arms and the leg.
    </p>

    <br>
    <div class="allegrolower">
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/content/stadium_cal_v2.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>Walking with a dynamic arm swing</p>
            </div>
        </div>
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/content/PSR_1.mp4" type="video/mp4">
            </video>
        </div>
    </div>

    <h1>In-Context Adaptation</h1>
    <p>
    We study the ability of our controller to recover from foot-trapping that occurs when one of the robot legs hits a discrete step obstacle (left). Note that steps or other form of discrete obstacles were not seen during training. This setting is relevant since our robot is blind and may find itself in such situations during deployment. We find that our controller is still able to detect and react to foot-trapping events based on the history of observations and actions. Specifically, after hitting the step with its leg the robot will attempt to lift its legs higher and faster on subsequent attempts.
    </p>
    <p>
    We command the robot to walk forward over a terrain with three sections: flat, downward slope, and flat again (right). We observe that our controller adapts its behavior based on terrain, changing the gait from natural walking on flat, to small steps on downward slope, to natural walking on flat again. This behavior is emergent and has not been pre-specified during training.
    </p>

    <br>
    <div class="allegrolower">
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/content/step_v2_1.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>Recovery from foot-trapping (left) and gait change based on terrain type (right)</p>
            </div>
        </div>
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/content/downslope_v2_1.mp4" type="video/mp4">
            </video>
        </div>
    </div>

    <h1>External Disturbances</h1>

    <p>
    Finally, we test the robustness of policies to sudden external forces. These experiments include pushing the robot with a wooden stick (left) and throwing a large yoga ball at the robot (right). We find that our controller is able to stabilize the robot in both of these scenarios.
    </p>
    <div class="teleop">
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/content/stick.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>Robustness to external disturbances</p>
            </div>
        </div>
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/content/yoga_ball.mp4" type="video/mp4">
            </video>
        </div>
    </div>

    <h1>BibTeX</h1>
    <p class="bibtex">
        @article{RealHumanoid2023,<br>
        &nbsp;&nbsp;title={Real-World Humanoid Locomotion with Reinforcement Learning},<br>
        &nbsp;&nbsp;author={Ilija Radosavovic and Tete Xiao and Bike Zhang and Trevor Darrell and Jitendra Malik and Koushil Sreenath},<br>
        &nbsp;&nbsp;year={2023},<br>
        &nbsp;&nbsp;journal={arXiv:2303.03381}<br>
        }
    </p>
</div>
<script type="text/javascript">
    /* https://stackoverflow.com/questions/3027707/how-to-change-the-playing-speed-of-videos-in-html5 */
    document.querySelector('video').defaultPlaybackRate = 1.0;
    document.querySelector('video').play();

    var videos =document.querySelectorAll('video');
    for (var i=0;i<1;i++)
    {
        videos[i].playbackRate = 1.0;
    }
</script>
<script>
    /* https://stackoverflow.com/questions/21163756/html5-and-javascript-to-play-videos-only-when-visible */
    var videos = document.getElementsByTagName("video");

    function checkScroll() {
        var fraction = 0.5; // Play when 70% of the player is visible.

        for(var i = 0; i < 1; i++) {  // only apply to the first video

            var video = videos[i];

            var x = video.offsetLeft, y = video.offsetTop, w = video.offsetWidth, h = video.offsetHeight, r = x + w, //right
                b = y + h, //bottom
                visibleX, visibleY, visible;

            visibleX = Math.max(0, Math.min(w, window.pageXOffset + window.innerWidth - x, r - window.pageXOffset));
            visibleY = Math.max(0, Math.min(h, window.pageYOffset + window.innerHeight - y, b - window.pageYOffset));

            visible = visibleX * visibleY / (w * h);

            if (visible > fraction) {
                video.play();
            } else {
                video.pause();
            }

        }

    }
    window.addEventListener('scroll', checkScroll, false);
    window.addEventListener('resize', checkScroll, false);
</script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        // Function to check if the user is on a mobile device
        function isMobileDevice() {
            return /Mobi|Android/i.test(navigator.userAgent);
        }
        // If the user is on a mobile device, disable autoplay
        if (isMobileDevice()) {
            const videos = document.querySelectorAll('video');
            videos.forEach(video => {
                video.autoplay = false;
                video.controls = true;
            });
        }
    });
</script>
<script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=51e0d73d83d06baa7a00000f"
        type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
        crossorigin="anonymous"></script>
<script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd002feec.js"
        type="text/javascript"></script>
</body>
</html>
